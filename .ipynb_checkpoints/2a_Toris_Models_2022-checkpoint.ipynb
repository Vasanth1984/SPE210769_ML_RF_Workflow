{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.spe.org/events/en/2022/conference/22apog/asia-pacific-oil-and-gas-conference-and-exhibition.html\"><img src = \"https://www.spe.org/binaries/content/gallery/specms/speevents/organization-logos/spe-logo-2020.png\" width = 200> \n",
    "\n",
    "<h1 align=center><font size = 5>Prediction of Recovery Factor using Machine Learning Methods</font></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=center><font size = 4> Munish Kumar, Kannapan Swaminathan</font></h1>\n",
    "<h1 align=center><font size = 4> Part B: Modelling of Recovery Factor</font></h1>\n",
    "<h1 align=center><font size = 3> ERCE 2022 </font></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. https://www.kaggle.com/code/kkhandekar/an-introduction-to-pycaret/notebook.\n",
    "2. https://towardsdatascience.com/5-things-you-dont-know-about-pycaret-528db0436eec\n",
    "3. https://www.dataquest.io/blog/understanding-regression-error-metrics/ \n",
    "4. https://www.analyticsvidhya.com/blog/2021/07/automl-using-pycaret-with-a-regression-use-case/\n",
    "5. https://www.datacamp.com/community/tutorials/guide-for-automating-ml-workflows-using-pycaret\n",
    "6. https://pycaret.readthedocs.io/en/latest/api/regression.html\n",
    "7. http://www.pycaret.org/tutorials/html/REG102.html\n",
    "8. https://githubhelp.com/ray-project/tune-sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check PyCaret Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycaret.utils import version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only install the following libraries if you dont have it, otherwise leave it commented out\n",
    "\n",
    "#!conda install -c anaconda natsort --yes\n",
    "#!conda install -c anaconda xlrd --yes\n",
    "\n",
    "#!pip install natsort --user\n",
    "#!pip install xlrd --user\n",
    "#!pip install pycaret[full] --user\n",
    "#!pip install mlflow --user\n",
    "#!pip install tune-sklearn ray[tune] --user\n",
    "#!pip install optuna -- user\n",
    "#!pip install hyperopt --user\n",
    "#!pip install redis --user\n",
    "\n",
    "# General Libraries\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from matplotlib.ticker import NullFormatter\n",
    "import time\n",
    "import re\n",
    "import requests\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import os\n",
    "import glob\n",
    "import sys\n",
    "from natsort import natsorted\n",
    "sns.set()\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Sklearn Liraries\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import datetime\n",
    "from datetime import timedelta, date \n",
    "start = time.time()\n",
    "%matplotlib inline\n",
    "\n",
    "import ray\n",
    "from ray import tune\n",
    "\n",
    "# Forces the print statement to show everything and not truncate\n",
    "# np.set_printoptions(threshold=sys.maxsize) \n",
    "print('Libraries imported')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Receive Data\n",
    "dir_name = r'C:\\Users\\kswaminathan\\OneDrive\\01_KannaLibrary\\15_Analogs'\n",
    "#dir_name = r'C:\\Users\\mkumar\\Documents\\GitHub\\@Papers\\SPE2022\\Final\\1_TORIS_MODEL'\n",
    "filename_suffix = 'csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Read in the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skiprows = 0\n",
    "#Means read in the ',' as thousand seperator. Also drops all columns which are unnamed.\n",
    "df = pd.read_excel(\"dftorisv2.xlsx\", thousands=',', skiprows = skiprows)\n",
    "df = df.loc[:, ~df.columns.str.contains('^Unnamed')] \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot as Heat map to check for highly correlated variables\n",
    "plt.figure(figsize=(15, 15))\n",
    "ax = sns.heatmap(df.corr(), annot=True, fmt=\".2f\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In observing the heat map above, I define highly correlated variables as having collinearity coeeficients of > 0.7. There was no highly correlated values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Convert to float - to ensure it is a numerical feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_drop = df.copy()\n",
    "df_drop = df_drop[df_drop['URF']<0.51].astype(float)\n",
    "df_drop = df_drop.astype(float)\n",
    "\n",
    "# Confirm properties of final dataframe\n",
    "print(len(df_drop))\n",
    "print(df_drop.info())\n",
    "print(df_drop.describe(include='all'))\n",
    "print(df_drop.columns.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final Data set has 389 rows and 24 columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train, Validation, and Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a mask where values that are true go into the training/test set\n",
    "# Note that I done it so that the random number is predictable\n",
    "\n",
    "msk = np.random.seed(0)\n",
    "msk = np.random.rand(len(df_drop))<0.8\n",
    "\n",
    "raw_train_validate_set = df_drop[msk]\n",
    "raw_test_set = df_drop[~msk]\n",
    "\n",
    "print(raw_train_validate_set.shape)\n",
    "print(raw_test_set.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_train_validate_set.to_excel(r'dfssoil.xlsx', index = False, header=True)\n",
    "raw_test_set.to_excel(r'BlindTest_SSOIL.xlsx', index = False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split the data set 80-20 into a \"train-validate\" set and a \"test\" set. The test set is external asn will never be seen by the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Pycaret Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pycaret will be used in the machine learning portion. Pycaret is a low-code machine learning library in Python that automates machine learning workflows. One of its key benefits is its ability to run a large number of differnt machine learning algorithms, but with only a few lines of code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skiprows = 0\n",
    "#Means read in the ',' as thousand seperator. Also drops all columns which are unnamed.\n",
    "df = pd.read_excel(\"dfssoil.xlsx\", thousands=',', skiprows = skiprows)\n",
    "df = df.loc[:, ~df.columns.str.contains('^Unnamed')] \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycaret.regression import *\n",
    "\n",
    "#Create a copy\n",
    "model_df = df_drop.copy()\n",
    "target = 'URF'\n",
    "\n",
    "# no resampling\n",
    "clf_none = setup(\n",
    "            data=model_df,\n",
    "            target=target,\n",
    "            session_id=42,\n",
    "            normalize=True,\n",
    "            transformation = True,\n",
    "            ignore_low_variance=True,\n",
    "            remove_outliers = True, #outliers_threshold = 0.1,\n",
    "            #remove_multicollinearity = True, multicollinearity_threshold = 0.6,\n",
    "            train_size=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best = compare_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "top3_fold_5 = compare_models(include=['rf', 'catboost', 'knn'], fold = 5, sort='MAE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "top3 = compare_models(include=['rf', 'catboost', 'knn'], fold = 10, sort='MAE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a performance improvement in going from 5 folds to 10 folds for all 3 models. To keep computation time reasonable, folds is kept at 10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Plot each Model and Check Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Random Forest (RFR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfr = create_model('rf')\n",
    "rfr_results = pull()\n",
    "\n",
    "rfr_feature_imp = pd.DataFrame({'Feature': get_config('X_train').columns, 'Value' : abs(rfr.feature_importances_)}).sort_values(by='Value', ascending=False)\n",
    "rfr_feature_imp.to_csv('Feature_importance_RFR.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(rfr, plot = 'feature')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Catboost Regressor (catboost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb = create_model('catboost')\n",
    "cb_results = pull()\n",
    "#print(gbr_results)\n",
    "\n",
    "cb_feature_imp = pd.DataFrame({'Feature': get_config('X_train').columns, 'Value' : abs(cb.feature_importances_)}).sort_values(by='Value', ascending=False)\n",
    "cb_feature_imp.to_csv('Feature_importance_CatBoost.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(cb, plot = 'feature')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Gradient Boost Regressor (gbr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbr = create_model('gbr')\n",
    "gbr_results = pull()\n",
    "#print(cb_results)\n",
    "\n",
    "gbr_feature_imp = pd.DataFrame({'Feature': get_config('X_train').columns, 'Value' : abs(gbr.feature_importances_)}).sort_values(by='Value', ascending=False)\n",
    "gbr_feature_imp.to_csv('Feature_importance_GBR.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given the sheer number of variables, will only plot the first 10\n",
    "# 'feature_all' will plot everything\n",
    "plot_model(gbr, plot = 'feature')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN Model - note that it does not support feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = create_model('knn')\n",
    "knn_results = pull()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3a. Testing for Optimisation - Not necessary to run\n",
    "-----------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the important parameters in the hyperparameters is the number of iterations over which the K fold cross validation is done. \n",
    "\n",
    "2 checks are done for this. The first scenario is over the range(0, 1000, 50). The optimisation ran overnight and showed that the ML algorithm did not see much improvement past 50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# elapsed = []\n",
    "# MAE_mean_iter = []\n",
    "# MSE_mean_iter = []\n",
    "# RMSE_mean_iter = []\n",
    "\n",
    "# # The output from the (0, 1000, 50) is saved; there is no need to run this again. \n",
    "# # Line has been modified just so the code can run.\n",
    "# for i in range(0, 51, 50):\n",
    "#     start = time.time()\n",
    "#     if i == 0:\n",
    "#         i += 1    \n",
    "#     tuned_cb = tune_model(cb, optimize = 'MSE', n_iter = i)\n",
    "#     #print(tuned_cb)\n",
    "#     MAE_mean_iter.append(pull()['MAE']['Mean'])\n",
    "#     MSE_mean_iter.append(pull()['MSE']['Mean'])\n",
    "#     RMSE_mean_iter.append(pull()['RMSE']['Mean'])\n",
    "#     elapsed.append((time.time() - start))\n",
    "\n",
    "# MAE_Mean = pd.DataFrame(MAE_mean_iter, index = elapsed, columns=['MAE Mean Error'])\n",
    "# MAE_Mean.index.name = 'Elapsed Time'\n",
    "\n",
    "# MSE_Mean = pd.DataFrame(MSE_mean_iter, index = elapsed, columns=['MSE Mean Error']) \n",
    "# MSE_Mean.index.name = 'Elapsed Time'\n",
    "\n",
    "# RMSE_Mean = pd.DataFrame(RMSE_mean_iter, index = elapsed, columns=['RMSE Mean Error'])\n",
    "# RMSE_Mean.index.name = 'Elapsed Time'\n",
    "\n",
    "# res_50_iter = pd.concat([MAE_Mean, MSE_Mean, RMSE_Mean], axis=1)\n",
    "\n",
    "# print(res_50_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b = sns.lineplot(data=res_50_iter)\n",
    "# b.axes.set_title(\"Error as function of Elapsed Time\",fontsize=20)\n",
    "# b.set_xlabel(\"Elapsed Time\",fontsize=20)\n",
    "# b.set_ylabel(\"Error\",fontsize=20)\n",
    "# #b.set_yscale('log')\n",
    "# b.tick_params(labelsize=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#res.to_csv('Run_Catboost_1000_Itr.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# elapsed = []\n",
    "# MAE_mean_iter = []\n",
    "# MSE_mean_iter = []\n",
    "# RMSE_mean_iter = []\n",
    "\n",
    "# # This was run at (1, 51, 1) to get increments of 1\n",
    "# # Right now, this is changed to (1, 51, 50) to allow the code to run efficiently\n",
    "# for i in range(1, 51, 50):\n",
    "#     start = time.time()\n",
    "#     tuned_cb = tune_model(cb, optimize = 'MSE', n_iter = i)\n",
    "#     MAE_mean_iter.append(pull()['MAE']['Mean'])\n",
    "#     MSE_mean_iter.append(pull()['MSE']['Mean'])\n",
    "#     RMSE_mean_iter.append(pull()['RMSE']['Mean'])\n",
    "#     elapsed.append((time.time() - start))\n",
    "\n",
    "# MAE_Mean = pd.DataFrame(MAE_mean_iter, index = elapsed, columns=['MAE Mean Error'])\n",
    "# MAE_Mean.index.name = 'Elapsed Time'\n",
    "\n",
    "# MSE_Mean = pd.DataFrame(MSE_mean_iter, index = elapsed, columns=['MSE Mean Error']) \n",
    "# MSE_Mean.index.name = 'Elapsed Time'\n",
    "\n",
    "# RMSE_Mean = pd.DataFrame(RMSE_mean_iter, index = elapsed, columns=['RMSE Mean Error'])\n",
    "# RMSE_Mean.index.name = 'Elapsed Time'\n",
    "\n",
    "# res_1_iter = pd.concat([MAE_Mean, MSE_Mean, RMSE_Mean], axis=1)\n",
    "\n",
    "# print(res_1_iter)\n",
    "\n",
    "# res_1_iter.to_csv('Run_Catboost_50_Itr.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b = sns.lineplot(data=res_1_iter)\n",
    "# b.axes.set_title(\"Error as function of Elapsed Time\",fontsize=20)\n",
    "# b.set_xlabel(\"Elapsed Time\",fontsize=20)\n",
    "# b.set_ylabel(\"Error\",fontsize=20)\n",
    "# #b.set_yscale('log')\n",
    "# b.tick_params(labelsize=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tuned_cb3 = tune_model(cb, optimize = 'RMSE', n_iter = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_model(tuned_cb3, plot = 'parameter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Gradient Boosting Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gbr = create_model('gbr')\n",
    "# print(gbr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tuned_gbr = tune_model(gbr, search_library = \"tune-sklearn\", search_algorithm=\"hyperopt\", optimize=\"RMSE\", n_iter=50)\n",
    "# print(tuned_gbr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Optimisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Tune the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The earlier experiments allow one to determine which model performs efficiently, and the tuning needed to arrive at the answer. Here, we will create the 3 specific models , which we will than blend, and than finally produce a \"tuned\" blended model based on earlier optimised parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from pycaret.distributions import UniformDistribution, CategoricalDistribution\n",
    "\n",
    "catboost_param_dists = {\n",
    "    'iterations': CategoricalDistribution([500,100,300]),\n",
    "    'colsample_bylevel': UniformDistribution(0.5, 1.0),\n",
    "    'random_strength': CategoricalDistribution([0,0.1,0.2,1,10]),\n",
    "    'max_depth' : CategoricalDistribution([5,6,7,8,9])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_models = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = create_model('rf', fold = 10)\n",
    "# rf = tune_model(rf, \n",
    "#                 optimize = 'RMSE', \n",
    "#                 n_iter = 50, \n",
    "#                 choose_better = True, \n",
    "#                  #search_library = \"tune-sklearn\", \n",
    "#                  #search_algorithm=\"Hyperopt\",\n",
    "#                  #search_algorithm=\"Optuna\",\n",
    "#                  #search_algorithm=\"bayesian\",\n",
    "#                 )\n",
    "tuned_models.append(rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb = create_model('catboost', fold = 10)\n",
    "# cb = tune_model(cb, \n",
    "#                 optimize = 'RMSE', \n",
    "#                 n_iter = 50, \n",
    "#                 choose_better = True,\n",
    "#                 #search_library = \"optuna\", \n",
    "#                 #search_library = \"tune-sklearn\", \n",
    "#                 #search_algorithm=\"bayesian\",\n",
    "#                 #search_algorithm=\"hyperopt\",\n",
    "#                 #custom_grid = catboost_param_dists ,\n",
    "#                 #early_stopping = \"asha\",\n",
    "#                 #early_stopping_max_iters = 10,\n",
    "#                 #return_tuner = False ,   \n",
    "#                )\n",
    "tuned_models.append(cb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = create_model('knn', fold = 10)\n",
    "# knn = tune_model(knn, \n",
    "#                  optimize = 'R2', \n",
    "#                  n_iter = 300, \n",
    "#                  choose_better = True, \n",
    "#                  #search_library = \"tune-sklearn\", \n",
    "#                  #search_algorithm=\"Hyperopt\",\n",
    "#                  #search_algorithm=\"Optuna\",\n",
    "#                  #search_algorithm=\"bayesian\",\n",
    "#                 )\n",
    "tuned_models.append(knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Ensemble the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pycaret.regression.ensemble_model(estimator, method: str = 'Bagging', fold: Optional[Union[int, Any]] = None, n_estimators: int = 10, round: int = 4, choose_better: bool = False, optimize: str = 'R2', fit_kwargs: Optional[dict] = None, groups: Optional[Union[str, Any]] = None, verbose: bool = True, return_train_score: bool = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_model = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_bagged_rf = ensemble_model(estimator = rf, method = 'Bagging', n_estimators=50, optimize = 'RMSE')\n",
    "prediction_model.append(tuned_bagged_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_boosted_rf = ensemble_model(estimator = rf, method = 'Boosting', n_estimators=50, optimize = 'RMSE')\n",
    "prediction_model.append(tuned_boosted_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_bagged_cb = ensemble_model(estimator = cb, method = 'Bagging', n_estimators=50, optimize = 'RMSE')\n",
    "prediction_model.append(tuned_bagged_cb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_boosted_cb = ensemble_model(estimator = cb, method = 'Boosting', n_estimators=50, optimize = 'RMSE')\n",
    "prediction_model.append(tuned_boosted_cb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_bagged_knn = ensemble_model(estimator = knn, method = 'Bagging', n_estimators=50, optimize = 'RMSE')\n",
    "prediction_model.append(tuned_bagged_knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_boosted_knn = ensemble_model(estimator = knn, method = 'Boosting', n_estimators=50, optimize = 'RMSE')\n",
    "prediction_model.append(tuned_boosted_knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. Blending all Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pycaret.regression.blend_models(\n",
    "estimator_list: list, fold: Optional[Union[int, Any]] = None, round: int = 4, choose_better: bool = False, optimize: str = 'R2', weights: Optional[List[float]] = None, fit_kwargs: Optional[dict] = None, groups: Optional[Union[str, Any]] = None, verbose: bool = True, return_train_score: bool = False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blend_5_soft = blend_models(estimator_list = tuned_models, fold=5, optimize = 'RMSE', choose_better = True)\n",
    "prediction_model.append(blend_5_soft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blend_10_soft = blend_models(estimator_list = tuned_models, fold=10, optimize = 'RMSE', choose_better = True)\n",
    "prediction_model.append(blend_10_soft)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d. Stacking all Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pycaret.regression.stack_models(estimator_list: list, meta_model=None, meta_model_fold: Optional[Union[int, Any]] = 5, fold: Optional[Union[int, Any]] = None, round: int = 4, restack: bool = True, choose_better: bool = False, optimize: str = 'R2', fit_kwargs: Optional[dict] = None, groups: Optional[Union[str, Any]] = None, verbose: bool = True, return_train_score: bool = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_5 = stack_models(estimator_list = tuned_models, meta_model = rf, fold = 5, optimize = 'RMSE', choose_better= True)\n",
    "prediction_model.append(stack_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_10 = stack_models(estimator_list = tuned_models, meta_model = rf, fold = 10, optimize = 'RMSE', choose_better= True)\n",
    "prediction_model.append(stack_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in prediction_model:\n",
    "    print(model.__class__.__name__)\n",
    "    display(predict_model(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the results above, the best models (largest R2) are \n",
    "\n",
    "1. tuned_bagged_knn\n",
    "2. blend_5_soft\n",
    "3. rf\n",
    "4. tuned_bagged_cb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Finalise the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_blend = finalize_model(blend_5_soft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_cb = finalize_model(tuned_bagged_cb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_knn = finalize_model(tuned_bagged_knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_rf = finalize_model(rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(tuned_bagged_knn, 'Bagged_KNN_15072022')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(blend_5_soft, 'Blended_model_15072022')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(rf, 'Ori_rf_15072022')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(tuned_bagged_cb, 'Bagged_cb_15072022')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plots to analyse Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = final_cb\n",
    "predict_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction Error \n",
    "plot_model(model, plot = 'error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cooks Distance Plot\n",
    "plot_model(model, plot='cooks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning Curve\n",
    "plot_model(model, plot='learning')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manifold Learning\n",
    "plot_model(model, plot='manifold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Hyperparameter\n",
    "plot_model(model, plot='parameter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Blind Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfblind = pd.read_excel(\"BlindTest_TORIS.xlsx\", thousands=',', skiprows = skiprows)\n",
    "#dfblind = dfblind.loc[:, ~df.columns.str.contains('^Unnamed')] \n",
    "dfblind.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BlindPredict = predict_model(final_blend, data=dfblind, round=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BlindPredict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = BlindPredict['URF']\n",
    "b = BlindPredict['Label']\n",
    "\n",
    "plt.figure(figsize=(14, 8))\n",
    "plt.scatter(a, b, color='blue')\n",
    "plt.plot(a, a, color = 'red', label = 'x=y')\n",
    "plt.xlabel(\"Recovery Factor (%)\", size=14)\n",
    "plt.ylabel(\"Evaluated Recovery Factor (%)\", size=14)\n",
    "\n",
    "#plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 'Completed Process'\n",
    "elapsed = (time.time() - start)\n",
    "print (\"%s in %s seconds\" % (count,elapsed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
